"""
Quality Control Module for MAKE-KI Reports
=========================================

This module provides comprehensive validation logic for KI status reports
generated by the Make-KI platform. A report is represented as a dictionary
containing rendered HTML sections (e.g., executive summary, quick wins,
roadmap) alongside KPI values used to validate ROI and benchmark
calculations. The validation process applies a series of structural,
content, data and compliance checks. Each check produces a `QualityCheck`
with a pass/fail flag, a numeric score and optional contextual details.

The overall quality of a report is summarised through a severity-weighted
score and categorised into a discrete `QualityLevel`. A convenience wrapper
`generate_quality_assured_report` is provided for legacy integrations and
adds a boolean ``ready_for_delivery`` flag indicating whether a report
passes a given threshold.
"""

from __future__ import annotations

import re
import logging
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum

logger = logging.getLogger("quality_control")


class Severity(Enum):
    """Severity levels used to weight quality checks."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


class QualityLevel(Enum):
    """Discrete quality levels mapped from a numeric score."""
    EXCELLENT = "EXCELLENT"
    GOOD = "GOOD"
    ACCEPTABLE = "ACCEPTABLE"
    NEEDS_IMPROVEMENT = "NEEDS_IMPROVEMENT"
    FAILED = "FAILED"


@dataclass
class QualityCheck:
    """A single quality check result."""
    name: str
    passed: bool
    score: float
    message: str
    severity: Severity
    category: str = "general"
    details: Dict[str, Any] = field(default_factory=dict)


class ReportQualityController:
    """
    Comprehensive quality control for AI status reports.

    This class encapsulates a set of structural, content and data checks that
    are applied to a rendered report. Each check records a `QualityCheck`
    entry with a pass/fail flag, a numeric score and metadata. The overall
    report score is computed as a severity-weighted average of all individual
    check scores.
    """

    def __init__(self) -> None:
        # A list to accumulate individual QualityCheck results
        self.checks: List[QualityCheck] = []
        # Threshold for the number of critical issues permitted
        self.critical_threshold: int = 3
        # Minimum overall score to deem a report acceptable
        self.min_score: float = 50.0

    def validate_complete_report(self, report_data: Dict[str, Any], lang: str = "de") -> Dict[str, Any]:
        """
        Validate a complete report by running all defined checks.

        Parameters
        ----------
        report_data : Dict[str, Any]
            The rendered report structured as a dictionary of HTML sections
            and KPI values.
        lang : str, optional
            The language code used for localisation hints in certain checks.

        Returns
        -------
        Dict[str, Any]
            A dictionary summarising the validation, including whether
            critical issues exist, the overall quality level, scores,
            detailed check results and improvement hints.
        """
        # Reset checks for each validation run
        self.checks = []

        # Structural integrity
        self._check_required_sections(report_data)
        self._check_html_validity(report_data)

        # Content quality
        self._check_executive_summary(report_data)
        self._check_quick_wins(report_data)
        self._check_roadmap(report_data)
        self._check_risks(report_data)
        self._check_compliance(report_data)

        # Data quality
        self._check_roi_plausibility(report_data)
        self._check_kpi_consistency(report_data)
        self._check_benchmark_validity(report_data)

        # Compliance
        self._check_regulatory_completeness(report_data)
        self._check_data_protection(report_data)

        # Aggregate scoring
        overall_score: float = self._calculate_score()
        quality_level: QualityLevel = self._determine_quality_level(overall_score)
        critical_issues: List[QualityCheck] = self._get_critical_issues()

        return {
            "passed": len(critical_issues) == 0 and overall_score >= self.min_score,
            "quality_level": quality_level.value,
            "overall_score": overall_score,
            "checks": self.checks,
            "critical_issues": critical_issues,
            "passed_checks": sum(1 for c in self.checks if c.passed),
            "total_checks": len(self.checks),
            "improvements": self._get_improvements(),
            "report_card": self._generate_report_card(overall_score, quality_level),
        }

    # ------------------------------------------------------------------
    # Individual checks
    # ------------------------------------------------------------------

    def _check_required_sections(self, data: Dict[str, Any]) -> None:
        """Ensure that all mandatory report sections are present and non‑trivial."""
        required = [
            "exec_summary_html",
            "quick_wins_html",
            "roadmap_html",
            "risks_html",
            "recommendations_html",
        ]
        for section in required:
            content = data.get(section, "")
            passed: bool = bool(content and len(content) > 50)
            self.checks.append(QualityCheck(
                name=f"section_{section}",
                passed=passed,
                score=100.0 if passed else 0.0,
                message=f"Section {section}: {'Present' if passed else 'Missing or too short'}",
                severity=Severity.HIGH if not passed else Severity.INFO,
                category="structure",
            ))

    def _check_html_validity(self, data: Dict[str, Any]) -> None:
        """Validate basic HTML structure and detect problematic constructs."""
        html_sections = [
            data.get("exec_summary_html", ""),
            data.get("quick_wins_html", ""),
            data.get("roadmap_html", ""),
            data.get("risks_html", ""),
            data.get("recommendations_html", ""),
        ]
        issues: List[str] = []
        for html in html_sections:
            if not html:
                continue
            # Avoid embedded scripts
            if "<script" in html.lower():
                issues.append("Contains script tags")
            # Ensure some structural tags are present
            if not re.search(r'<(p|ul|ol|table|div)', html, flags=re.IGNORECASE):
                issues.append("Missing proper HTML structure")
            # Balance opening and closing tags (simple heuristic)
            open_tags = re.findall(r'<(\w+)[^>]*>', html)
            close_tags = re.findall(r'</(\w+)>', html)
            if len(open_tags) != len(close_tags):
                issues.append("Unclosed HTML tags detected")
        passed: bool = len(issues) == 0
        self.checks.append(QualityCheck(
            name="html_validity",
            passed=passed,
            score=100.0 if passed else 50.0,
            message="HTML validation: " + (issues[0] if issues else "Valid"),
            severity=Severity.MEDIUM if not passed else Severity.INFO,
            category="technical",
            details={"issues": issues},
        ))

    def _check_executive_summary(self, data: Dict[str, Any]) -> None:
        """Assess the quality of the executive summary section."""
        summary: str = data.get("exec_summary_html", "") or ""
        checks: Dict[str, bool] = {
            "length": 200 <= len(summary) <= 2000,
            "has_roi": "roi" in summary.lower() or "return" in summary.lower(),
            "has_score": "score" in summary.lower() or "%" in summary,
            "has_recommendation": any(word in summary.lower() for word in ["empfehl", "recommend", "fokus", "focus"]),
        }
        score: float = sum(100.0 for passed in checks.values() if passed) / len(checks)
        passed: bool = score >= 75.0
        self.checks.append(QualityCheck(
            name="executive_summary_quality",
            passed=passed,
            score=score,
            message=f"Executive Summary: {score:.0f}% quality score",
            severity=Severity.HIGH if not passed else Severity.INFO,
            category="content",
            details=checks,
        ))

    def _check_quick_wins(self, data: Dict[str, Any]) -> None:
        """Evaluate the quick wins section for sufficiency and completeness."""
        content: str = data.get("quick_wins_html", "") or ""
        # Count list items as proxy for actionable points
        items: int = len(re.findall(r'<li[^>]*>', content))
        has_effort: bool = ("tag" in content.lower() or "day" in content.lower())
        has_owner: bool = ("owner" in content.lower() or "verantwort" in content.lower())
        score: float = 0.0
        if items >= 5:
            score += 40.0
        elif items >= 3:
            score += 25.0
        if has_effort:
            score += 30.0
        if has_owner:
            score += 30.0
        passed: bool = score >= 70.0
        self.checks.append(QualityCheck(
            name="quick_wins_completeness",
            passed=passed,
            score=score,
            message=f"Quick Wins: {items} items, effort: {has_effort}, owner: {has_owner}",
            severity=Severity.MEDIUM if not passed else Severity.INFO,
            category="content",
        ))

    def _check_roadmap(self, data: Dict[str, Any]) -> None:
        """Assess the roadmap for defined phases and milestones."""
        content: str = data.get("roadmap_html", "") or ""
        # Recognise typical phase markers
        phases: List[str] = ["W1", "W2", "W3", "W5", "W9", "Woche", "Week", "Phase"]
        phase_count: int = sum(1 for p in phases if p in content)
        has_milestones: bool = len(re.findall(r'<li[^>]*>', content)) >= 4
        score: float = min(100.0, (phase_count / 4.0) * 50.0 + (50.0 if has_milestones else 0.0))
        passed: bool = score >= 60.0
        self.checks.append(QualityCheck(
            name="roadmap_structure",
            passed=passed,
            score=score,
            message=f"Roadmap: {phase_count} phases defined, milestones: {has_milestones}",
            severity=Severity.MEDIUM if not passed else Severity.INFO,
            category="content",
        ))

    def _check_risks(self, data: Dict[str, Any]) -> None:
        """Validate the presence and completeness of the risk matrix."""
        content: str = data.get("risks_html", "") or ""
        has_table: bool = "<table" in content.lower()
        rows: int = len(re.findall(r'<tr[^>]*>', content))
        has_probability: bool = ("wahrsch" in content.lower() or "probability" in content.lower())
        has_impact: bool = ("auswirkung" in content.lower() or "impact" in content.lower())
        has_mitigation: bool = ("mitigation" in content.lower() or "maßnahm" in content.lower())
        score: float = 0.0
        if has_table and rows >= 5:
            score += 40.0
        if has_probability:
            score += 20.0
        if has_impact:
            score += 20.0
        if has_mitigation:
            score += 20.0
        passed: bool = score >= 70.0
        self.checks.append(QualityCheck(
            name="risk_matrix_completeness",
            passed=passed,
            score=score,
            message=f"Risk Matrix: {rows} risks, complete: {all([has_probability, has_impact, has_mitigation])}",
            severity=Severity.HIGH if not passed else Severity.INFO,
            category="content",
        ))

    def _check_compliance(self, data: Dict[str, Any]) -> None:
        """Check coverage of key regulations in the recommendations section."""
        content: str = data.get("recommendations_html", "") or ""
        regulations: List[str] = ["AI Act", "DSGVO", "GDPR", "DSA", "CRA", "Data Act"]
        mentioned: int = sum(1 for reg in regulations if reg.lower() in content.lower())
        score: float = min(100.0, (mentioned / 3.0) * 100.0)
        passed: bool = mentioned >= 2
        self.checks.append(QualityCheck(
            name="compliance_coverage",
            passed=passed,
            score=score,
            message=f"Compliance: {mentioned} regulations mentioned",
            severity=Severity.CRITICAL if not passed else Severity.INFO,
            category="compliance",
        ))

    def _check_roi_plausibility(self, data: Dict[str, Any]) -> None:
        """Verify the plausibility of ROI calculations based on investment and savings."""
        investment: float = float(data.get("roi_investment", 0) or 0.0)
        savings: float = float(data.get("roi_annual_saving", 0) or 0.0)
        payback: float = float(data.get("kpi_roi_months", 0) or 0.0)
        if investment > 0.0 and savings > 0.0:
            calc_payback: float = (investment / savings) * 12.0
            deviation: float = abs(calc_payback - payback) / calc_payback if calc_payback > 0.0 else 1.0
            passed: bool = deviation < 0.1  # allow 10% deviation
            score: float = max(0.0, 100.0 * (1.0 - deviation))
        else:
            passed = False
            score = 0.0
        self.checks.append(QualityCheck(
            name="roi_calculation",
            passed=passed,
            score=score,
            message=f"ROI plausibility: {'Consistent' if passed else 'Inconsistent calculations'}",
            severity=Severity.HIGH if not passed else Severity.INFO,
            category="data",
            details={"investment": investment, "savings": savings, "payback": payback},
        ))

    def _check_kpi_consistency(self, data: Dict[str, Any]) -> None:
        """Assess the correlation between KPIs like automation and efficiency."""
        compliance: float = float(data.get("kpi_compliance", 0) or 0.0)
        automation: float = float(data.get("automatisierungsgrad", 0) or 0.0)
        efficiency: float = float(data.get("kpi_efficiency", 0) or 0.0)
        # Expect efficiency to decline as automation increases; simple linear expectation
        expected_efficiency: float = max(0.0, 100.0 - automation) * 0.8
        deviation: float = abs(efficiency - expected_efficiency)
        passed: bool = deviation < 30.0  # 30% tolerance
        score: float = max(0.0, 100.0 - deviation)
        self.checks.append(QualityCheck(
            name="kpi_consistency",
            passed=passed,
            score=score,
            message=f"KPI consistency: {score:.0f}% aligned",
            severity=Severity.MEDIUM if not passed else Severity.INFO,
            category="data",
        ))

    def _check_benchmark_validity(self, data: Dict[str, Any]) -> None:
        """Placeholder for validating benchmark inputs; always passes currently."""
        passed: bool = True
        score: float = 100.0
        self.checks.append(QualityCheck(
            name="benchmark_validity",
            passed=passed,
            score=score,
            message="Benchmark data validated",
            severity=Severity.INFO,
            category="data",
        ))

    def _check_regulatory_completeness(self, data: Dict[str, Any]) -> None:
        """Ensure a data protection officer (DPO) is assigned."""
        readiness: str = str(data.get("readiness_level", "") or "")
        has_dpo: bool = data.get("datenschutzbeauftragter", "").lower() in ["ja", "extern", "intern", "yes"]
        passed: bool = has_dpo
        score: float = 100.0 if passed else 0.0
        self.checks.append(QualityCheck(
            name="regulatory_requirements",
            passed=passed,
            score=score,
            message=f"Regulatory: DPO assigned: {has_dpo}",
            severity=Severity.CRITICAL if not passed else Severity.INFO,
            category="compliance",
        ))

    def _check_data_protection(self, data: Dict[str, Any]) -> None:
        """Placeholder for data protection checks; always passes currently."""
        passed: bool = True
        score: float = 100.0
        self.checks.append(QualityCheck(
            name="data_protection",
            passed=passed,
            score=score,
            message="Data protection measures in place",
            severity=Severity.INFO,
            category="compliance",
        ))

    # ------------------------------------------------------------------
    # Scoring helpers
    # ------------------------------------------------------------------

    def _calculate_score(self) -> float:
        """Compute the severity-weighted average score across all checks."""
        if not self.checks:
            return 0.0
        weights: Dict[Severity, float] = {
            Severity.CRITICAL: 3.0,
            Severity.HIGH: 2.0,
            Severity.MEDIUM: 1.5,
            Severity.LOW: 1.0,
            Severity.INFO: 0.5,
        }
        total_weight: float = sum(weights.get(c.severity, 1.0) for c in self.checks)
        weighted_score: float = sum(c.score * weights.get(c.severity, 1.0) for c in self.checks)
        return weighted_score / total_weight if total_weight > 0.0 else 0.0

    def _determine_quality_level(self, score: float) -> QualityLevel:
        """Map the numeric score to a discrete quality level."""
        if score >= 90.0:
            return QualityLevel.EXCELLENT
        if score >= 75.0:
            return QualityLevel.GOOD
        if score >= 60.0:
            return QualityLevel.ACCEPTABLE
        if score >= 40.0:
            return QualityLevel.NEEDS_IMPROVEMENT
        return QualityLevel.FAILED

    def _get_critical_issues(self) -> List[QualityCheck]:
        """Return all checks that failed with critical or high severity."""
        return [c for c in self.checks if not c.passed and c.severity in (Severity.CRITICAL, Severity.HIGH)]

    def _get_improvements(self) -> List[str]:
        """Generate a list of up to five improvement recommendations."""
        improvements: List[str] = []
        for check in self.checks:
            if not check.passed:
                if check.category == "structure":
                    improvements.append(f"Add missing or expand section: {check.name}")
                elif check.category == "content":
                    improvements.append(f"Improve {check.name}: {check.message}")
                elif check.category == "compliance":
                    improvements.append(f"Address compliance gap: {check.message}")
                elif check.category == "data":
                    improvements.append(f"Verify data accuracy: {check.message}")
        return improvements[:5]

    def _generate_report_card(self, score: float, level: QualityLevel) -> Dict[str, Any]:
        """Create a concise summary of the quality assessment for display."""
        passed_checks: int = sum(1 for c in self.checks if c.passed)
        total_checks: int = len(self.checks)
        critical_count: int = len(self._get_critical_issues())
        return {
            "grade": level.value,
            "score": f"{score:.1f}/100",
            "passed_checks": f"{passed_checks}/{total_checks}",
            "critical_issues": critical_count,
            "timestamp": None,  # Caller may set a timestamp if needed
        }


# ------------------------------------------------------------------------
# Module-level convenience wrapper
# ------------------------------------------------------------------------

def generate_quality_assured_report(report_data: Dict[str, Any], lang: str = "de", threshold: float = 80.0) -> Dict[str, Any]:
    """
    Run the quality checks against a rendered report and add a convenience
    flag ``ready_for_delivery`` indicating if the report meets the given
    threshold. This wrapper is provided for compatibility with earlier
    integrations and hides the instantiation of :class:`ReportQualityController`.

    Parameters
    ----------
    report_data : Dict[str, Any]
        The rendered report to validate. Expected keys include the individual
        HTML sections and KPI values referenced by the quality checks.
    lang : str, optional
        Language code forwarded to the validation methods.
    threshold : float, optional
        Minimum overall score required to consider the report fit for delivery.

    Returns
    -------
    Dict[str, Any]
        The full quality assessment result extended with a boolean flag
        ``ready_for_delivery`` summarising the outcome relative to ``threshold``.
    """
    qc = ReportQualityController()
    result = qc.validate_complete_report(report_data, lang=lang)
    result["ready_for_delivery"] = result.get("overall_score", 0.0) >= threshold
    return result